---
title: "lbb-classification-2"
author: "Alfan"
date: "2/28/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro

## Background

Telemarketing is a method of direct marketing which a person (can be sales) prospective customers to buy products or services, either over the phone or through face to face or web conferencing appointment. Telemarketing can also include recoreded sales pithes programmed to be played over the phone by automatic dialing.

Bank is one of the organisation use telemarketing method for selling banking products or services. telemarketing is a popular method used by bank to selling, because bank products and services sometimes too complicated for some users to understand. It more easy to users or target user to understand products or service if it explain directly. One advanteage of telemarketing by person, target users can directly asking question, if they didnt understand something.

Nowdays, Telemarketing has been negatively associated with various scams and frauds, such as pyramid schemes, and with deceptively overpriced products and services. Fraudulent telemarketing companies are frequently referred to as "telemarketing boiler rooms" or simply "boiler rooms". Telemarketing is often criticized as an unethical business practice due to the perception of high-pressure sales techniques during unsolicited calls. Telemarketers marketing telephone companies may participate in telephone slamming, the practice of switching a customer's telephone service without their knowledge or authorization.

Bank as financing organisation really care about good reputation and good branding, and one of bad thing do telemarketing can interfere reputation it self. So we need find out which our target will not buy product or service if bank offer product or service using telemarketing. It can help protect bank reputation by not disturbing target that we already know will not buy the product.

## Analysis Method

In this case we will use machine learning to understand pattern and predict classification or label, we use several predictive model to predict using training and testing data. Predictive model we use is, Naive Bayes Classifier, Decision Tree, and Random Forest. 

We will compare the result of predction and see the performance from each mode. This 3 model are categorized as supervised learning. Supervised learning popular to predict pattern, this pattern can learn from train data and do ETL (Extract Transform Load) to get feature information. Based from feature we will compare with clasification patter from model get from labeled data to get final prediction.

# Data Preparation

## Import Library

```{r message=FALSE}
# Data wrangling Library
library(tidyverse)
library(dplyr) 

# Visualize data
library(ggplot2)
library(inspectdf)
library(GGally)
library(plotly)

# Naive Bayes 
library(e1071)

# Splitting Data
library(rsample)

# Random Forest
library(randomForest)

# Smote for unbalanced data
library(DMwR)

# ROCR
library(ROCR)

# Confussion Matrix
library(caret)

# Decision Tree
library(partykit)
```


## Import Function 

```{r}
source("matrix_result.R")
source("metrics.R")
```


## Read Data

Telemarketing dataset was obtained from UCI Machine Learning Repository, The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.

```{r message=FALSE}
telemark <- read_csv2("data/bank-full.csv")
```
 
```{r}
glimpse(telemark)
```

Column Description:   
1. `age`: age (numeric)   
2. `job` : type of job (categorical: "admin.","unknown","unemployed","management","housemaid","entrepreneur","student", "blue-collar","self-employed","retired","technician","services")  
3. `marital` : marital status (categorical: "married","divorced","single"; note: "divorced" means divorced or widowed)   
4. `education` : education (categorical: "unknown","secondary","primary","tertiary")   
5. `default`: has credit in default? (binary: "yes","no")   
6. `balance`: average yearly balance, in euros (numeric)   
7. `housing`: has housing loan? (binary: "yes","no")   
8. `loan`: has personal loan? (binary: "yes","no")   
9. `contact`: contact communication type (categorical: "unknown","telephone","cellular")    
10. `day`: last contact day of the month (numeric)   
11. `month`: last contact month of year (categorical: "jan", "feb", "mar", ..., "nov", "dec")   
12. `duration`: last contact duration, in seconds (numeric)   
13. `campaign`: number of contacts performed during this campaign and for this client (numeric, includes last contact)   
14. `pdays`: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)   
15. `previous`: number of contacts performed before this campaign and for this client (numeric)   
16. `poutcome`: outcome of the previous marketing campaign (categorical: "unknown","other","failure","success")   
17. `y`: has the client subscribed a term deposit? (binary: "yes","no")

# Data Wrangling

Missing Value (NA) is general problem from dataaset, there's some way to solve the problem. Based on several refrence say that there is NO good way to deal with missing data. So before we going forward to next step, we should check missing value.

```{r}
table(is.na(telemark))
```

The data has no missing value, so we doesnt need any thing with missing value. Based on column description some our imported variables have incorrect data types. Change the data type refer to column description.

```{r}
telemark <- telemark %>% 
  mutate(job = as.factor(job),
         marital = as.factor(marital),
         education = as.factor(education),
         default = as.factor(default),
         housing = as.factor(housing),
         loan = as.factor(loan),
         contact = as.factor(contact),
         month = as.factor(month),
         poutcome = as.factor(poutcome),
         subscribe = as.factor(y)) %>% 
  select(-c(y))
```

There's some of data variables is numeric, we can visualize the histogram to get data distribution from them. 

```{r}
numericCols <- unlist(lapply(telemark, is.numeric))
show_plot(inspect_num(telemark[,numericCols]))
```

Our target variables before are "Y" and we change it to "subscribe", it make us more easy to understand which observer that want to subscribe any product or services when get call from telemarketing. 

```{r}
levels(telemark$subscribe)
```

Our target variables consist of 2 levels "yes" means users agree to subscribe or buy product and "no" means users didnt agree or reject offers from telemarketing. Lets take look at the overall data structure, 

```{r}
summary(telemark)
```

# Exploratory Data Analysis

When conducting a supervised classification with machine learning algorithms such as Random Forests, one recommended practice is to work with a balanced classification dataset. Check proportion of our target variables `subscribe`

```{r}
prop.table(table(telemark$subscribe))
```

We found that our target variable `subsribe` is Imbalanced. It mean data refers to a situation where the number of observations is not the same for all the classes in a classification dataset. To avoid loss of variance, we will use upsampling to balance the proportion.

Lets take look correlation between predictor variables

```{r}
show_plot(inspect_cor(subset(telemark, select = -c(subscribe))))
```

```{r}
ggcorr(telemark, label = T)
```

Two plots above explain, there are some predictor variables have correlation with other predictor variable. These variables are `previous` with `pdays`, `campaign` with `day`, and `balance` with `age`. This didnt make decision us to take out the variables from dataset, but only warn maybe it can cause some models will not work porperly like Naive Bayes.

# Cross Validation

Cross-validation (CV) is a statistical method that can be used to evaluate the performance of models or algorithms where the data is separated into two subsets namely learning process data and validation / evaluation data. In this case we will seperate data with proportion 80% dataset for data training and rest 20% we use as data test.


```{r}
set.seed(1)
split <- initial_split(data = telemark, prop = 0.8, strata = subscribe)
telemark_train <- training(split)
telemark_test <- testing(split)
```



```{r}
prop.table(table(telemark_train$subscribe))
```

```{r}
telemark_train_upsample <- SMOTE(subscribe ~ ., as.data.frame(telemark_train), perc.over = 100, perc.under = 200)
```

```{r}
prop.table(table(telemark_train_upsample$subscribe))
```


```{r}
model_naive <- naiveBayes(subscribe ~ ., data = telemark_train_upsample)
```

```{r}
naive_prediction <- predict(model_naive, telemark_test)
naive_prediction_raw <- as.data.frame(predict(model_naive, telemark_test, type = "raw"))

naive_prediction_raw <- naive_prediction_raw %>%
  mutate(no = round(no,4),
         yes = round(yes,4))
```


```{r}
naive_matrix <- confusionMatrix(naive_prediction, telemark_test$subscribe, positive = "yes")
table <- as.table(naive_matrix)
table <- as.data.frame(table)

table %>% ggplot(aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), fontface = "bold", color = "white") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
naive_matrix <- matrix_result(naive_matrix, "Naive Bayes")
naive_matrix
```

```{r}
# ROC
naive_roc <- data.frame(prediction = naive_prediction_raw[,2],
                        trueclass = as.numeric(telemark_test$subscribe=="yes"))
head(naive_roc)
```


```{r}
naive_roc_pred <- prediction(naive_roc$prediction, naive_roc$trueclass) 

# ROC curve
plot(performance(naive_roc_pred, "tpr", "fpr"),
     main = "ROC")
abline(a = 0, b = 1)
```

```{r}
# AUC
auc_ROCR_n <- performance(naive_roc_pred, measure = "auc")
auc_ROCR_n <- auc_ROCR_n@y.values[[1]]
auc_ROCR_n
```



```{r}
# model tuning - metrics function
co <- seq(0.01,0.99,length=100)
result <- matrix(0,100,4)

# apply function metrics
for(i in 1:100){
  result[i,] = metrics(cutoff = co[i], 
                     prob = naive_prediction_raw$yes, 
                     ref = as.factor(ifelse(telemark_test$subscribe == "yes", 1, 0)), 
                     postarget = "1", 
                     negtarget = "0")
}

# visualize
ggplotly(tibble("Recall" = result[,1],
           "Accuracy" = result[,2],
           "Precision" = result[,3],
           "Specificity" = result[,4],
                   "Cutoff" = co) %>% 
  gather(key = "Metrics", value = "value", 1:4) %>% 
  ggplot(aes(x = Cutoff, y = value, col = Metrics)) +
  geom_line(lwd = 1.5) +
  scale_color_manual(values = c("darkred","darkgreen","orange", "blue")) +
  scale_y_continuous(breaks = seq(0,1,0.1), limits = c(0,1)) +
  scale_x_continuous(breaks = seq(0,1,0.1)) +
  labs(title = "Tradeoff Model Perfomance") +
  theme_minimal() +
  theme(legend.position = "top",
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank()))
```

```{r}
#Tuning Threshold
naive_prediction_tuning <- naive_prediction_raw %>%
  mutate(label = as.factor(ifelse(yes >= 0.65, "yes", "no"))) %>% 
  select(label)
```


```{r}
naive_matrix_tuning <- confusionMatrix(naive_prediction_tuning$label, naive_prediction, positive = "yes")
naive_matrix <- matrix_result(naive_matrix_tuning, "Naive Bayes Tuning")
naive_matrix
```

```{r}
model_dtree <- ctree(subscribe ~ ., telemark_train_upsample)
```

```{r}
dtree_prediction <- predict(model_dtree, telemark_test)
```

```{r}
dtree_matrix <- confusionMatrix(dtree_prediction, telemark_test$subscribe, positive = "yes")
dtree_matrix <- matrix_result(dtree_matrix, "Decision Tree")
dtree_matrix
```

```{r}
model_dtree_tuning <- ctree(subscribe ~ ., telemark_train_upsample,
                            control = ctree_control(mincriterion = 0.1, minsplit = 100, minbucket = 60))

dtree_prediction_tuning <- predict(model_dtree_tuning, telemark_test)
```

```{r}
dtree_matrix_tuning <- confusionMatrix(dtree_prediction_tuning, telemark_test$subscribe)
dtree_matrix_tuning <- matrix_result(dtree_matrix_tuning, "Decision Tree Tuning")
dtree_matrix_tuning
```

```{r}
ctrl <- trainControl(method = "repeatedcv", number = 5,repeats = 3)
```

```{r}
# model_rforest <- train(subscribe ~ ., data = telemark_train_upsample, method = "rf", trControl = ctrl, ntree = 100)
# saveRDS(model_rforest, file = "model_rforest.RDS")
model_rforest <- readRDS("model_rforest.RDS")
```

```{r}
rforest_predict <- predict(model_rforest, telemark_test)
rforest_predict_raw <- predict(model_rforest, telemark_test, type = "prob")
```

```{r}
plot(model_rforest$finalModel)
legend("topright", colnames(model_rforest$finalModel$err.rate),col=1:6,cex=0.8,fill=1:6)
```


```{r}
rforest_matrix <- confusionMatrix(rforest_predict, telemark_test$subscribe, positive = "yes")
table <- as.table(rforest_matrix)
table <- as.data.frame(table)

table %>% ggplot(aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), fontface = "bold", color = "white") +
  theme_minimal() +
  theme(legend.position = "none")
```


```{r}
rforest_matrix <- matrix_result(rforest_matrix, "Random Forest")
rforest_matrix
```


```{r}
# ROC
forest_roc <- data.frame(prediction = rforest_predict_raw[,2],
                        trueclass = as.numeric(telemark_test$subscribe=="yes"))
head(forest_roc)
```


```{r}
forest_rocz_prediction <- prediction(forest_roc$prediction, forest_roc$trueclass) 

# ROC curve
plot(performance(forest_rocz_prediction, "tpr", "fpr"),
     main = "ROC")
abline(a = 0, b = 1)
```

```{r}
# AUC
auc_ROCR_n <- performance(forest_rocz_prediction, measure = "auc")
auc_ROCR_n <- auc_ROCR_n@y.values[[1]]
auc_ROCR_n
```



```{r}
co <- seq(0.01,0.99,length=100)
result <- matrix(0,100,4)

# apply function metrics
for(i in 1:100){
  result[i,] = metrics(cutoff = co[i], 
                     prob = rforest_predict_raw$yes, 
                     ref = as.factor(ifelse(telemark_test$subscribe == "yes", 1, 0)), 
                     postarget = "1", 
                     negtarget = "0")
}

# visualize
ggplotly(tibble("Recall" = result[,1],
           "Accuracy" = result[,2],
           "Precision" = result[,3],
           "Specificity" = result[,4],
                   "Cutoff" = co) %>% 
  gather(key = "Metrics", value = "value", 1:4) %>% 
  ggplot(aes(x = Cutoff, y = value, col = Metrics)) +
  geom_line(lwd = 1.5) +
  scale_color_manual(values = c("darkred","darkgreen","orange", "blue")) +
  scale_y_continuous(breaks = seq(0,1,0.1), limits = c(0,1)) +
  scale_x_continuous(breaks = seq(0,1,0.1)) +
  labs(title = "Tradeoff Model Perfomance") +
  theme_minimal() +
  theme(legend.position = "top",
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank()))
```


```{r}
#Tuning Threshold
rforest_predict_tuning <- rforest_predict_raw %>%
  mutate(label = as.factor(ifelse(yes >= 0.52, "yes", "no"))) %>% 
  select(label)
```


```{r}
rforest_matrix_tuning <- confusionMatrix(naive_prediction_tuning$label, naive_prediction, positive = "yes")
rforest_matrix_tuning <- matrix_result(rforest_matrix_tuning, "Random Forest Tuning")
rforest_matrix_tuning
```



